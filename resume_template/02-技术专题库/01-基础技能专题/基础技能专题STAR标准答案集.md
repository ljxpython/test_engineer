# åŸºç¡€æŠ€èƒ½ä¸“é¢˜STARæ ‡å‡†ç­”æ¡ˆé›†

## ğŸ“š è¯´æ˜
æœ¬æ–‡æ¡£ä¸º01-åŸºç¡€æŠ€èƒ½ä¸“é¢˜æä¾›å®Œæ•´çš„STARæ¡†æ¶æ ‡å‡†ç­”æ¡ˆï¼Œè¡¥å……åŸæœ‰é¢˜ç›®ä¸­ç¼ºå¤±çš„ç»“æ„åŒ–å›ç­”ã€‚

---

## ğŸ Pythonç¼–ç¨‹ä¸“é¢˜ STARç­”æ¡ˆ

### â­â­â­ è§£é‡ŠPythonä¸­çš„è£…é¥°å™¨åŠå…¶åº”ç”¨

**é—®é¢˜**: è§£é‡ŠPythonä¸­çš„è£…é¥°å™¨æœºåˆ¶ï¼Œå¹¶ä¸¾ä¾‹è¯´æ˜åœ¨æµ‹è¯•å¼€å‘ä¸­çš„åº”ç”¨ï¼Ÿ

**STARæ¡†æ¶å›ç­”**:

**Situation (æƒ…æ™¯)**: 
åœ¨æµ‹è¯•æ¡†æ¶å¼€å‘ä¸­ï¼Œæˆ‘ç»å¸¸éœ€è¦ä¸ºæµ‹è¯•æ–¹æ³•æ·»åŠ é€šç”¨åŠŸèƒ½ï¼Œå¦‚æ‰§è¡Œæ—¶é—´ç»Ÿè®¡ã€å¼‚å¸¸æ•è·ã€æµ‹è¯•æ•°æ®æ¸…ç†ç­‰ã€‚å¦‚æœæ¯ä¸ªæµ‹è¯•æ–¹æ³•éƒ½å•ç‹¬å®ç°è¿™äº›åŠŸèƒ½ï¼Œä¼šå¯¼è‡´å¤§é‡é‡å¤ä»£ç ã€‚

**Task (ä»»åŠ¡)**: 
éœ€è¦æ‰¾åˆ°ä¸€ç§ä¼˜é›…çš„æ–¹å¼æ¥ä¸ºå¤šä¸ªå‡½æ•°æ·»åŠ ç›¸åŒçš„åŠŸèƒ½ï¼ŒåŒæ—¶ä¿æŒä»£ç çš„å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

**Action (è¡ŒåŠ¨)**: 
æˆ‘ä½¿ç”¨Pythonè£…é¥°å™¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼š

```python
import time
import functools
import logging
from typing import Callable, Any

# 1. æ‰§è¡Œæ—¶é—´ç»Ÿè®¡è£…é¥°å™¨
def timing_decorator(func: Callable) -> Callable:
    """ç»Ÿè®¡å‡½æ•°æ‰§è¡Œæ—¶é—´çš„è£…é¥°å™¨"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        execution_time = end_time - start_time
        logging.info(f"{func.__name__} executed in {execution_time:.4f} seconds")
        return result
    return wrapper

# 2. å¼‚å¸¸é‡è¯•è£…é¥°å™¨
def retry(max_attempts: int = 3, delay: float = 1.0):
    """å¼‚å¸¸é‡è¯•è£…é¥°å™¨"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts - 1:
                        raise e
                    logging.warning(f"Attempt {attempt + 1} failed: {e}, retrying in {delay}s")
                    time.sleep(delay)
        return wrapper
    return decorator

# 3. æµ‹è¯•æ•°æ®æ¸…ç†è£…é¥°å™¨
def cleanup_test_data(func: Callable) -> Callable:
    """æµ‹è¯•åæ¸…ç†æ•°æ®çš„è£…é¥°å™¨"""
    @functools.wraps(func)
    def wrapper(self, *args, **kwargs):
        try:
            result = func(self, *args, **kwargs)
            return result
        finally:
            # æ¸…ç†æµ‹è¯•æ•°æ®
            self.cleanup_resources()
            logging.info(f"Test data cleaned up for {func.__name__}")
    return wrapper

# 4. å‚æ•°éªŒè¯è£…é¥°å™¨
def validate_parameters(**validators):
    """å‚æ•°éªŒè¯è£…é¥°å™¨"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # éªŒè¯å‚æ•°
            for param_name, validator in validators.items():
                if param_name in kwargs:
                    if not validator(kwargs[param_name]):
                        raise ValueError(f"Invalid parameter {param_name}: {kwargs[param_name]}")
            return func(*args, **kwargs)
        return wrapper
    return decorator

# åœ¨æµ‹è¯•ç±»ä¸­çš„åº”ç”¨
class APITestCase:
    @timing_decorator
    @retry(max_attempts=3, delay=2.0)
    @cleanup_test_data
    def test_user_login_api(self):
        """ç”¨æˆ·ç™»å½•æ¥å£æµ‹è¯•"""
        response = self.send_login_request()
        assert response.status_code == 200
        assert "token" in response.json()
        
    @validate_parameters(
        user_id=lambda x: isinstance(x, int) and x > 0,
        email=lambda x: "@" in x
    )
    def test_user_update_api(self, user_id: int, email: str):
        """ç”¨æˆ·æ›´æ–°æ¥å£æµ‹è¯•"""
        payload = {"user_id": user_id, "email": email}
        response = self.send_update_request(payload)
        assert response.status_code == 200
```

**Result (ç»“æœ)**: 
é€šè¿‡ä½¿ç”¨è£…é¥°å™¨ï¼Œæˆ‘å®ç°äº†ï¼š
1. **ä»£ç å¤ç”¨æ€§æå‡90%**: é€šç”¨åŠŸèƒ½åªéœ€è¦å†™ä¸€æ¬¡ï¼Œå¯ä»¥åº”ç”¨åˆ°å¤šä¸ªæµ‹è¯•æ–¹æ³•
2. **ç»´æŠ¤æˆæœ¬é™ä½60%**: ä¿®æ”¹é€šç”¨åŠŸèƒ½æ—¶åªéœ€ä¿®æ”¹è£…é¥°å™¨ï¼Œä¸éœ€è¦ä¿®æ”¹æ¯ä¸ªä½¿ç”¨çš„æ–¹æ³•
3. **ä»£ç å¯è¯»æ€§å¢å¼º**: æµ‹è¯•æ–¹æ³•çš„æ ¸å¿ƒé€»è¾‘æ›´åŠ æ¸…æ™°ï¼Œè¾…åŠ©åŠŸèƒ½é€šè¿‡è£…é¥°å™¨æ˜ç¡®æ ‡è¯†
4. **æµ‹è¯•ç¨³å®šæ€§æå‡**: ç»Ÿä¸€çš„é‡è¯•å’Œå¼‚å¸¸å¤„ç†æœºåˆ¶è®©æµ‹è¯•æ›´åŠ ç¨³å®š

### â­â­â­ è¯´è¯´Pythonä¸­çš„å¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹

**é—®é¢˜**: è¯·è§£é‡ŠPythonä¸­GILçš„å½±å“ï¼Œä»¥åŠå¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹çš„åº”ç”¨åœºæ™¯ï¼Ÿ

**STARæ¡†æ¶å›ç­”**:

**Situation (æƒ…æ™¯)**: 
åœ¨æ€§èƒ½æµ‹è¯•é¡¹ç›®ä¸­ï¼Œæˆ‘éœ€è¦æ¨¡æ‹Ÿå¤§é‡å¹¶å‘ç”¨æˆ·è®¿é—®ç³»ç»Ÿï¼Œå•çº¿ç¨‹æ‰§è¡Œæ•ˆç‡å¤ªä½ï¼Œéœ€è¦ä½¿ç”¨å¹¶å‘ç¼–ç¨‹æ¥æå‡æµ‹è¯•æ‰§è¡Œæ•ˆç‡ã€‚

**Task (ä»»åŠ¡)**: 
éœ€è¦é€‰æ‹©åˆé€‚çš„å¹¶å‘æ–¹æ¡ˆï¼ˆå¤šçº¿ç¨‹ vs å¤šè¿›ç¨‹ï¼‰ï¼Œå¹¶è§£å†³Python GILå¸¦æ¥çš„æ€§èƒ½é™åˆ¶ã€‚

**Action (è¡ŒåŠ¨)**: 
æˆ‘æ·±å…¥ç ”ç©¶äº†Pythonçš„å¹¶å‘æœºåˆ¶å¹¶åˆ¶å®šäº†åº”ç”¨ç­–ç•¥ï¼š

```python
import threading
import multiprocessing
import asyncio
import time
import requests
import queue
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

# 1. GILå½±å“æ¼”ç¤º
def cpu_intensive_task(n: int) -> int:
    """CPUå¯†é›†å‹ä»»åŠ¡"""
    result = 0
    for i in range(n):
        result += i * i
    return result

def io_intensive_task(url: str) -> dict:
    """IOå¯†é›†å‹ä»»åŠ¡"""
    response = requests.get(url, timeout=5)
    return {"url": url, "status": response.status_code}

# 2. å¤šçº¿ç¨‹é€‚ç”¨åœºæ™¯ï¼šIOå¯†é›†å‹ä»»åŠ¡
class ThreadedAPITester:
    def __init__(self, max_workers: int = 10):
        self.max_workers = max_workers
        self.results = queue.Queue()
    
    def test_api_concurrency(self, urls: list, concurrent_users: int = 100):
        """ä½¿ç”¨å¤šçº¿ç¨‹æµ‹è¯•APIå¹¶å‘æ€§èƒ½"""
        print(f"Testing {len(urls)} APIs with {concurrent_users} concurrent users")
        
        start_time = time.time()
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = []
            for _ in range(concurrent_users):
                for url in urls:
                    future = executor.submit(self.single_api_request, url)
                    futures.append(future)
            
            # æ”¶é›†ç»“æœ
            results = []
            for future in futures:
                try:
                    result = future.result(timeout=30)
                    results.append(result)
                except Exception as e:
                    results.append({"error": str(e)})
        
        end_time = time.time()
        print(f"Completed {len(results)} requests in {end_time - start_time:.2f} seconds")
        return results
    
    def single_api_request(self, url: str) -> dict:
        """å•ä¸ªAPIè¯·æ±‚"""
        try:
            start = time.time()
            response = requests.get(url, timeout=10)
            duration = time.time() - start
            
            return {
                "url": url,
                "status_code": response.status_code,
                "response_time": duration,
                "success": True
            }
        except Exception as e:
            return {
                "url": url,
                "error": str(e),
                "success": False
            }

# 3. å¤šè¿›ç¨‹é€‚ç”¨åœºæ™¯ï¼šCPUå¯†é›†å‹ä»»åŠ¡
class ProcessedDataAnalyzer:
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or multiprocessing.cpu_count()
    
    def analyze_test_results(self, test_data_chunks: list):
        """ä½¿ç”¨å¤šè¿›ç¨‹åˆ†æå¤§é‡æµ‹è¯•æ•°æ®"""
        print(f"Analyzing data with {self.max_workers} processes")
        
        start_time = time.time()
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self.process_data_chunk, chunk) 
                      for chunk in test_data_chunks]
            
            results = []
            for future in futures:
                result = future.result()
                results.append(result)
        
        end_time = time.time()
        print(f"Data analysis completed in {end_time - start_time:.2f} seconds")
        return self.merge_results(results)
    
    @staticmethod
    def process_data_chunk(data_chunk: list) -> dict:
        """å¤„ç†å•ä¸ªæ•°æ®å—ï¼ˆCPUå¯†é›†å‹ï¼‰"""
        # æ¨¡æ‹Ÿå¤æ‚çš„æ•°æ®åˆ†æè®¡ç®—
        total_requests = len(data_chunk)
        success_count = sum(1 for item in data_chunk if item.get('success'))
        avg_response_time = sum(item.get('response_time', 0) for item in data_chunk) / total_requests
        
        # è®¡ç®—åˆ†ä½æ•°ï¼ˆCPUå¯†é›†å‹æ“ä½œï¼‰
        response_times = sorted([item.get('response_time', 0) for item in data_chunk])
        p95 = response_times[int(len(response_times) * 0.95)] if response_times else 0
        p99 = response_times[int(len(response_times) * 0.99)] if response_times else 0
        
        return {
            'total_requests': total_requests,
            'success_rate': success_count / total_requests,
            'avg_response_time': avg_response_time,
            'p95_response_time': p95,
            'p99_response_time': p99
        }
    
    def merge_results(self, results: list) -> dict:
        """åˆå¹¶å¤šä¸ªè¿›ç¨‹çš„åˆ†æç»“æœ"""
        total_requests = sum(r['total_requests'] for r in results)
        weighted_success_rate = sum(r['success_rate'] * r['total_requests'] for r in results) / total_requests
        weighted_avg_rt = sum(r['avg_response_time'] * r['total_requests'] for r in results) / total_requests
        
        return {
            'total_requests': total_requests,
            'overall_success_rate': weighted_success_rate,
            'overall_avg_response_time': weighted_avg_rt,
            'max_p95': max(r['p95_response_time'] for r in results),
            'max_p99': max(r['p99_response_time'] for r in results)
        }

# 4. å¼‚æ­¥ç¼–ç¨‹ï¼šåç¨‹æ–¹å¼
class AsyncAPITester:
    def __init__(self, max_concurrent: int = 100):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def test_api_async(self, urls: list):
        """ä½¿ç”¨å¼‚æ­¥ç¼–ç¨‹æµ‹è¯•API"""
        async with aiohttp.ClientSession() as session:
            tasks = [self.single_async_request(session, url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return results
    
    async def single_async_request(self, session, url: str):
        """å•ä¸ªå¼‚æ­¥APIè¯·æ±‚"""
        async with self.semaphore:
            try:
                start = time.time()
                async with session.get(url, timeout=10) as response:
                    duration = time.time() - start
                    return {
                        "url": url,
                        "status_code": response.status,
                        "response_time": duration,
                        "success": True
                    }
            except Exception as e:
                return {
                    "url": url,
                    "error": str(e),
                    "success": False
                }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # IOå¯†é›†å‹ä»»åŠ¡ä½¿ç”¨å¤šçº¿ç¨‹
    thread_tester = ThreadedAPITester(max_workers=20)
    api_urls = ["http://api.example.com/endpoint1", "http://api.example.com/endpoint2"]
    thread_results = thread_tester.test_api_concurrency(api_urls, concurrent_users=50)
    
    # CPUå¯†é›†å‹ä»»åŠ¡ä½¿ç”¨å¤šè¿›ç¨‹
    process_analyzer = ProcessedDataAnalyzer()
    data_chunks = [thread_results[i:i+100] for i in range(0, len(thread_results), 100)]
    analysis_result = process_analyzer.analyze_test_results(data_chunks)
    
    print("Performance Test Analysis:", analysis_result)
```

**Result (ç»“æœ)**:
1. **IOå¯†é›†å‹ä»»åŠ¡æ€§èƒ½æå‡**: ä½¿ç”¨å¤šçº¿ç¨‹è¿›è¡ŒAPIå¹¶å‘æµ‹è¯•ï¼Œæ€§èƒ½æå‡äº†10-15å€
2. **CPUå¯†é›†å‹ä»»åŠ¡çªç ´GIL**: ä½¿ç”¨å¤šè¿›ç¨‹è¿›è¡Œæ•°æ®åˆ†æï¼Œæ€§èƒ½æå‡äº†3-4å€ï¼ˆæ¥è¿‘CPUæ ¸å¿ƒæ•°ï¼‰
3. **èµ„æºåˆ©ç”¨ç‡ä¼˜åŒ–**: åˆç†é€‰æ‹©å¹¶å‘æ¨¡å¼ï¼Œç³»ç»Ÿèµ„æºåˆ©ç”¨ç‡æå‡äº†70%
4. **æµ‹è¯•æ‰§è¡Œæ—¶é—´ç¼©çŸ­**: æ•´ä½“æµ‹è¯•æ‰§è¡Œæ—¶é—´ä»2å°æ—¶ç¼©çŸ­åˆ°20åˆ†é’Ÿ

**å…³é”®å†³ç­–åŸåˆ™**:
- **IOå¯†é›†å‹ä»»åŠ¡**: ä½¿ç”¨å¤šçº¿ç¨‹æˆ–å¼‚æ­¥ç¼–ç¨‹ï¼ˆasyncioï¼‰
- **CPUå¯†é›†å‹ä»»åŠ¡**: ä½¿ç”¨å¤šè¿›ç¨‹
- **æ··åˆå‹ä»»åŠ¡**: æ ¹æ®ä¸»è¦ç“¶é¢ˆé€‰æ‹©ï¼Œæˆ–ä½¿ç”¨è¿›ç¨‹+çº¿ç¨‹çš„ç»„åˆ

---

## ğŸ’» Linuxç³»ç»Ÿä¸“é¢˜ STARç­”æ¡ˆ

### â­â­â­ å¦‚ä½•æ’æŸ¥Linuxç³»ç»Ÿæ€§èƒ½é—®é¢˜ï¼Ÿ

**é—®é¢˜**: åœ¨æµ‹è¯•ç¯å¢ƒä¸­å‘ç°LinuxæœåŠ¡å™¨æ€§èƒ½å¼‚å¸¸ï¼Œä½ ä¼šå¦‚ä½•ç³»ç»Ÿæ€§åœ°æ’æŸ¥é—®é¢˜ï¼Ÿ

**STARæ¡†æ¶å›ç­”**:

**Situation (æƒ…æ™¯)**: 
åœ¨ä¸€æ¬¡å‹åŠ›æµ‹è¯•ä¸­ï¼Œæˆ‘å‘ç°æµ‹è¯•ç¯å¢ƒçš„LinuxæœåŠ¡å™¨å“åº”å˜æ…¢ï¼ŒAPIå“åº”æ—¶é—´ä»å¹³å‡200msä¸Šå‡åˆ°2ç§’ä»¥ä¸Šï¼Œç³»ç»Ÿè´Ÿè½½å¼‚å¸¸ã€‚

**Task (ä»»åŠ¡)**: 
éœ€è¦å¿«é€Ÿå®šä½æ€§èƒ½ç“¶é¢ˆï¼Œæ‰¾å‡ºæ ¹æœ¬åŸå› ï¼Œå¹¶åˆ¶å®šè§£å†³æ–¹æ¡ˆæ¢å¤ç³»ç»Ÿæ€§èƒ½ã€‚

**Action (è¡ŒåŠ¨)**: 
æˆ‘æŒ‰ç…§ç³»ç»ŸåŒ–çš„æ’æŸ¥æ­¥éª¤è¿›è¡Œè¯Šæ–­ï¼š

```bash
#!/bin/bash
# Linuxç³»ç»Ÿæ€§èƒ½æ’æŸ¥è„šæœ¬

# 1. ç³»ç»Ÿæ•´ä½“çŠ¶æ€æ£€æŸ¥
echo "=== ç³»ç»Ÿæ•´ä½“çŠ¶æ€ ==="
uptime                          # ç³»ç»Ÿè´Ÿè½½å’Œè¿è¡Œæ—¶é—´
free -h                         # å†…å­˜ä½¿ç”¨æƒ…å†µ
df -h                          # ç£ç›˜ç©ºé—´ä½¿ç”¨
lscpu                          # CPUä¿¡æ¯

# 2. CPUä½¿ç”¨æƒ…å†µåˆ†æ
echo "=== CPUä½¿ç”¨åˆ†æ ==="
top -n 1 -b | head -20         # CPUä½¿ç”¨ç‡æ’åº
vmstat 1 5                     # è™šæ‹Ÿå†…å­˜ç»Ÿè®¡
mpstat -P ALL 1 5              # å¤šæ ¸CPUä½¿ç”¨æƒ…å†µ
pidstat -u 1 5                 # è¿›ç¨‹çº§CPUä½¿ç”¨

# 3. å†…å­˜ä½¿ç”¨æƒ…å†µåˆ†æ
echo "=== å†…å­˜ä½¿ç”¨åˆ†æ ==="
cat /proc/meminfo | grep -E 'MemTotal|MemFree|MemAvailable|Buffers|Cached|SwapTotal|SwapFree'
ps aux --sort=-%mem | head -10  # å†…å­˜ä½¿ç”¨æœ€é«˜çš„è¿›ç¨‹
pmap -d <pid>                   # ç‰¹å®šè¿›ç¨‹å†…å­˜æ˜ å°„
slabtop -o                      # å†…æ ¸å†…å­˜ä½¿ç”¨

# 4. ç£ç›˜I/Oåˆ†æ
echo "=== ç£ç›˜I/Oåˆ†æ ==="
iostat -x 1 5                  # ç£ç›˜I/Oç»Ÿè®¡
iotop -o -d 1                   # å®æ—¶ç£ç›˜I/Oç›‘æ§
lsof | grep deleted             # æŸ¥æ‰¾å·²åˆ é™¤ä½†ä»è¢«å ç”¨çš„æ–‡ä»¶

# 5. ç½‘ç»œè¿æ¥åˆ†æ
echo "=== ç½‘ç»œè¿æ¥åˆ†æ ==="
netstat -tulpn | grep LISTEN    # ç›‘å¬ç«¯å£
ss -tulpn                       # å¥—æ¥å­—ç»Ÿè®¡
iftop -t -s 10                  # ç½‘ç»œæµé‡ç›‘æ§
tcpdump -i any -n -c 100        # ç½‘ç»œåŒ…æ•è·

# 6. è¿›ç¨‹å’ŒæœåŠ¡åˆ†æ
echo "=== è¿›ç¨‹åˆ†æ ==="
ps aux --sort=-%cpu | head -10  # CPUä½¿ç”¨æœ€é«˜çš„è¿›ç¨‹
pstree -p                       # è¿›ç¨‹æ ‘
systemctl list-units --failed   # å¤±è´¥çš„æœåŠ¡
journalctl -p err -since today  # ä»Šå¤©çš„é”™è¯¯æ—¥å¿—
```

å…·ä½“æ’æŸ¥æ¡ˆä¾‹ï¼š

```python
import subprocess
import json
import time
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class SystemMetrics:
    timestamp: float
    cpu_usage: float
    memory_usage: float
    disk_io: Dict[str, float]
    network_io: Dict[str, float]
    load_average: List[float]

class LinuxPerformanceAnalyzer:
    def __init__(self):
        self.metrics_history = []
    
    def collect_system_metrics(self) -> SystemMetrics:
        """æ”¶é›†ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        
        # CPUä½¿ç”¨ç‡
        cpu_cmd = "top -bn1 | grep 'Cpu(s)' | awk '{print $2}' | cut -d'%' -f1"
        cpu_usage = float(subprocess.getoutput(cpu_cmd))
        
        # å†…å­˜ä½¿ç”¨ç‡
        mem_cmd = "free | grep Mem | awk '{printf \"%.2f\", $3/$2 * 100.0}'"
        memory_usage = float(subprocess.getoutput(mem_cmd))
        
        # ç³»ç»Ÿè´Ÿè½½
        load_cmd = "uptime | awk -F'load average:' '{print $2}' | tr ',' ' '"
        load_values = [float(x.strip()) for x in subprocess.getoutput(load_cmd).split()]
        
        # ç£ç›˜I/Oï¼ˆç®€åŒ–ç‰ˆï¼‰
        disk_io = self.get_disk_io_stats()
        
        # ç½‘ç»œI/O
        network_io = self.get_network_io_stats()
        
        return SystemMetrics(
            timestamp=time.time(),
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            disk_io=disk_io,
            network_io=network_io,
            load_average=load_values
        )
    
    def get_disk_io_stats(self) -> Dict[str, float]:
        """è·å–ç£ç›˜I/Oç»Ÿè®¡"""
        try:
            # è¯»å–/proc/diskstats
            cmd = "cat /proc/diskstats | grep -E 'sda|nvme' | head -1"
            output = subprocess.getoutput(cmd)
            fields = output.split()
            
            if len(fields) >= 14:
                return {
                    "reads_completed": float(fields[3]),
                    "writes_completed": float(fields[7]),
                    "read_sectors": float(fields[5]),
                    "write_sectors": float(fields[9])
                }
        except:
            pass
        return {"reads_completed": 0, "writes_completed": 0}
    
    def get_network_io_stats(self) -> Dict[str, float]:
        """è·å–ç½‘ç»œI/Oç»Ÿè®¡"""
        try:
            cmd = "cat /proc/net/dev | grep eth0 | awk '{print $2,$10}'"
            output = subprocess.getoutput(cmd)
            if output:
                rx_bytes, tx_bytes = output.split()
                return {
                    "rx_bytes": float(rx_bytes),
                    "tx_bytes": float(tx_bytes)
                }
        except:
            pass
        return {"rx_bytes": 0, "tx_bytes": 0}
    
    def analyze_performance_issue(self, duration: int = 300) -> Dict:
        """åˆ†ææ€§èƒ½é—®é¢˜"""
        print(f"å¼€å§‹æ€§èƒ½åˆ†æï¼ŒæŒç»­æ—¶é—´: {duration}ç§’")
        
        start_time = time.time()
        while time.time() - start_time < duration:
            metrics = self.collect_system_metrics()
            self.metrics_history.append(metrics)
            time.sleep(10)  # æ¯10ç§’é‡‡é›†ä¸€æ¬¡
        
        return self.generate_analysis_report()
    
    def generate_analysis_report(self) -> Dict:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not self.metrics_history:
            return {"error": "No metrics collected"}
        
        # è®¡ç®—å¹³å‡å€¼å’Œå³°å€¼
        avg_cpu = sum(m.cpu_usage for m in self.metrics_history) / len(self.metrics_history)
        max_cpu = max(m.cpu_usage for m in self.metrics_history)
        
        avg_memory = sum(m.memory_usage for m in self.metrics_history) / len(self.metrics_history)
        max_memory = max(m.memory_usage for m in self.metrics_history)
        
        avg_load = sum(m.load_average[0] for m in self.metrics_history) / len(self.metrics_history)
        max_load = max(m.load_average[0] for m in self.metrics_history)
        
        # æ€§èƒ½é—®é¢˜è¯Šæ–­
        issues = []
        recommendations = []
        
        # CPUé—®é¢˜æ£€æŸ¥
        if max_cpu > 80:
            issues.append(f"CPUä½¿ç”¨ç‡è¿‡é«˜: å³°å€¼{max_cpu:.1f}%")
            recommendations.append("æ£€æŸ¥CPUå¯†é›†å‹è¿›ç¨‹: ps aux --sort=-%cpu")
            
        # å†…å­˜é—®é¢˜æ£€æŸ¥
        if max_memory > 80:
            issues.append(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: å³°å€¼{max_memory:.1f}%")
            recommendations.append("æ£€æŸ¥å†…å­˜ä½¿ç”¨: ps aux --sort=-%mem")
            
        # ç³»ç»Ÿè´Ÿè½½æ£€æŸ¥
        cpu_cores = int(subprocess.getoutput("nproc"))
        if max_load > cpu_cores * 0.8:
            issues.append(f"ç³»ç»Ÿè´Ÿè½½è¿‡é«˜: å³°å€¼{max_load:.2f} (CPUæ ¸å¿ƒæ•°: {cpu_cores})")
            recommendations.append("åˆ†æè¿›ç¨‹ç­‰å¾…é˜Ÿåˆ—å’ŒI/Oæƒ…å†µ")
        
        return {
            "analysis_period": f"{len(self.metrics_history) * 10} seconds",
            "cpu_analysis": {
                "average": f"{avg_cpu:.1f}%",
                "peak": f"{max_cpu:.1f}%"
            },
            "memory_analysis": {
                "average": f"{avg_memory:.1f}%", 
                "peak": f"{max_memory:.1f}%"
            },
            "load_analysis": {
                "average": f"{avg_load:.2f}",
                "peak": f"{max_load:.2f}",
                "cpu_cores": cpu_cores
            },
            "identified_issues": issues,
            "recommendations": recommendations
        }

# å…·ä½“ä½¿ç”¨æ¡ˆä¾‹
def troubleshoot_performance():
    """æ€§èƒ½æ•…éšœæ’æŸ¥æµç¨‹"""
    
    print("å¼€å§‹ç³»ç»Ÿæ€§èƒ½æ•…éšœæ’æŸ¥...")
    
    # 1. å¿«é€Ÿæ£€æŸ¥ç³»ç»ŸçŠ¶æ€
    print("\n1. ç³»ç»Ÿå¿«é€Ÿæ£€æŸ¥")
    quick_checks = [
        ("ç³»ç»Ÿè´Ÿè½½", "uptime"),
        ("å†…å­˜ä½¿ç”¨", "free -h"),
        ("ç£ç›˜ç©ºé—´", "df -h"),
        ("CPUä½¿ç”¨", "top -bn1 | head -5")
    ]
    
    for name, cmd in quick_checks:
        print(f"\n{name}:")
        result = subprocess.getoutput(cmd)
        print(result)
    
    # 2. è¯¦ç»†æ€§èƒ½åˆ†æ
    print("\n2. è¯¦ç»†æ€§èƒ½åˆ†æ")
    analyzer = LinuxPerformanceAnalyzer()
    report = analyzer.analyze_performance_issue(duration=60)  # 1åˆ†é’Ÿé‡‡æ ·
    
    print(json.dumps(report, indent=2, ensure_ascii=False))
    
    # 3. é—®é¢˜å®šä½å’Œè§£å†³æ–¹æ¡ˆ
    if report.get("identified_issues"):
        print("\n3. å‘ç°çš„é—®é¢˜:")
        for issue in report["identified_issues"]:
            print(f"  - {issue}")
            
        print("\nå»ºè®®çš„è§£å†³æ–¹æ¡ˆ:")
        for rec in report["recommendations"]:
            print(f"  - {rec}")
    else:
        print("\n3. æœªå‘ç°æ˜æ˜¾çš„æ€§èƒ½é—®é¢˜")

if __name__ == "__main__":
    troubleshoot_performance()
```

**Result (ç»“æœ)**:
é€šè¿‡ç³»ç»ŸåŒ–çš„æ’æŸ¥ï¼Œæˆ‘å‘ç°äº†é—®é¢˜çš„æ ¹æœ¬åŸå› å¹¶æˆåŠŸè§£å†³ï¼š

1. **é—®é¢˜å®šä½**: å‘ç°æŸä¸ªJavaåº”ç”¨å­˜åœ¨å†…å­˜æ³„æ¼ï¼Œå¯¼è‡´é¢‘ç¹GCï¼ŒCPUä½¿ç”¨ç‡é£™å‡
2. **è§£å†³æ–¹æ¡ˆ**: é‡å¯æœ‰é—®é¢˜çš„æœåŠ¡ï¼Œå¹¶è°ƒæ•´JVMå‚æ•°ä¼˜åŒ–åƒåœ¾å›æ”¶
3. **æ€§èƒ½æ¢å¤**: APIå“åº”æ—¶é—´æ¢å¤åˆ°æ­£å¸¸çš„200msä»¥ä¸‹
4. **é¢„é˜²æªæ–½**: å»ºç«‹äº†æ€§èƒ½ç›‘æ§è„šæœ¬ï¼Œè‡ªåŠ¨åŒ–æ£€æµ‹å¼‚å¸¸å¹¶å‘é€å‘Šè­¦

è¿™æ¬¡æ’æŸ¥è®©æˆ‘æ€»ç»“äº†ä¸€å¥—æ ‡å‡†çš„Linuxæ€§èƒ½é—®é¢˜è¯Šæ–­æµç¨‹ï¼Œåç»­åº”ç”¨åˆ°äº†å¤šä¸ªé¡¹ç›®ä¸­ï¼Œå¤§å¤§æå‡äº†æ•…éšœå¤„ç†æ•ˆç‡ã€‚

---

## ğŸ—„ï¸ æ•°æ®åº“ä¸SQLä¸“é¢˜ STARç­”æ¡ˆ

### â­â­â­ å¦‚ä½•ä¼˜åŒ–æ…¢SQLæŸ¥è¯¢ï¼Ÿ

**é—®é¢˜**: åœ¨æµ‹è¯•æ•°æ®åº“æ€§èƒ½æ—¶å‘ç°æŸäº›SQLæŸ¥è¯¢å¾ˆæ…¢ï¼Œä½ ä¼šå¦‚ä½•ç³»ç»Ÿæ€§åœ°ä¼˜åŒ–è¿™äº›æŸ¥è¯¢ï¼Ÿ

**STARæ¡†æ¶å›ç­”**:

**Situation (æƒ…æ™¯)**: 
åœ¨ä¸€æ¬¡æ•°æ®åº“æ€§èƒ½æµ‹è¯•ä¸­ï¼Œæˆ‘å‘ç°ç”¨æˆ·æŸ¥è¯¢é¡µé¢çš„å“åº”æ—¶é—´è¶…è¿‡5ç§’ï¼Œé€šè¿‡ç›‘æ§å‘ç°æ˜¯åç«¯çš„ä¸€æ¡å¤æ‚SQLæŸ¥è¯¢æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼Œå½±å“äº†æ•´ä½“æ€§èƒ½ã€‚

**Task (ä»»åŠ¡)**: 
éœ€è¦åˆ†æè¿™æ¡æ…¢SQLçš„æ‰§è¡Œè®¡åˆ’ï¼Œæ‰¾å‡ºæ€§èƒ½ç“¶é¢ˆï¼Œå¹¶åˆ¶å®šä¼˜åŒ–ç­–ç•¥å°†æŸ¥è¯¢æ—¶é—´æ§åˆ¶åœ¨500msä»¥å†…ã€‚

**Action (è¡ŒåŠ¨)**:
æˆ‘é‡‡ç”¨äº†ç³»ç»ŸåŒ–çš„SQLä¼˜åŒ–æ–¹æ³•ï¼š

```sql
-- åŸå§‹æ…¢SQLç¤ºä¾‹ï¼ˆè®¢å•ç»Ÿè®¡æŸ¥è¯¢ï¼‰
SELECT 
    u.username,
    u.email,
    COUNT(o.order_id) as total_orders,
    SUM(o.total_amount) as total_amount,
    AVG(o.total_amount) as avg_order_amount,
    MAX(o.created_at) as last_order_date
FROM users u
LEFT JOIN orders o ON u.user_id = o.user_id
LEFT JOIN order_items oi ON o.order_id = oi.order_id
LEFT JOIN products p ON oi.product_id = p.product_id
WHERE u.created_at >= '2023-01-01'
    AND u.status = 'active'
    AND p.category_id IN (1,2,3,4,5)
GROUP BY u.user_id, u.username, u.email
HAVING COUNT(o.order_id) > 0
ORDER BY total_amount DESC
LIMIT 100;
```

**ç¬¬ä¸€æ­¥ï¼šæ‰§è¡Œè®¡åˆ’åˆ†æ**

```sql
-- MySQLæ‰§è¡Œè®¡åˆ’åˆ†æ
EXPLAIN ANALYZE SELECT ...;

-- PostgreSQLæ‰§è¡Œè®¡åˆ’åˆ†æ
EXPLAIN (ANALYZE, BUFFERS, VERBOSE) SELECT ...;

-- æŸ¥çœ‹è¯¦ç»†çš„æ‰§è¡Œç»Ÿè®¡
SHOW PROFILES;
SHOW PROFILE FOR QUERY 1;
```

**ç¬¬äºŒæ­¥ï¼šç´¢å¼•ä¼˜åŒ–ç­–ç•¥**

```sql
-- 1. åˆ†æç°æœ‰ç´¢å¼•
SHOW INDEX FROM users;
SHOW INDEX FROM orders;
SHOW INDEX FROM order_items;
SHOW INDEX FROM products;

-- 2. åˆ›å»ºå¤åˆç´¢å¼•ä¼˜åŒ–WHEREæ¡ä»¶
CREATE INDEX idx_users_created_status ON users(created_at, status);
CREATE INDEX idx_orders_user_created ON orders(user_id, created_at);
CREATE INDEX idx_products_category ON products(category_id);
CREATE INDEX idx_order_items_order_product ON order_items(order_id, product_id);

-- 3. è¦†ç›–ç´¢å¼•ä¼˜åŒ–ï¼ˆé¿å…å›è¡¨ï¼‰
CREATE INDEX idx_users_covering ON users(user_id, username, email, created_at, status);
```

**ç¬¬ä¸‰æ­¥ï¼šæŸ¥è¯¢é‡æ„ä¼˜åŒ–**

```sql
-- ä¼˜åŒ–æ–¹æ¡ˆ1ï¼šå­æŸ¥è¯¢æ‹†åˆ†
-- å…ˆè·å–ç¬¦åˆæ¡ä»¶çš„ç”¨æˆ·
WITH active_users AS (
    SELECT user_id, username, email
    FROM users 
    WHERE created_at >= '2023-01-01' 
        AND status = 'active'
),
-- è·å–æœ‰æ•ˆè®¢å•ï¼ˆé¿å…ä¸å¿…è¦çš„JOINï¼‰
valid_orders AS (
    SELECT DISTINCT o.user_id, o.order_id, o.total_amount, o.created_at
    FROM orders o
    INNER JOIN order_items oi ON o.order_id = oi.order_id
    INNER JOIN products p ON oi.product_id = p.product_id
    WHERE p.category_id IN (1,2,3,4,5)
)
SELECT 
    au.username,
    au.email,
    COUNT(vo.order_id) as total_orders,
    SUM(vo.total_amount) as total_amount,
    AVG(vo.total_amount) as avg_order_amount,
    MAX(vo.created_at) as last_order_date
FROM active_users au
INNER JOIN valid_orders vo ON au.user_id = vo.user_id
GROUP BY au.user_id, au.username, au.email
ORDER BY total_amount DESC
LIMIT 100;

-- ä¼˜åŒ–æ–¹æ¡ˆ2ï¼šEXISTSæ›¿ä»£IN
SELECT 
    u.username,
    u.email,
    COUNT(o.order_id) as total_orders,
    SUM(o.total_amount) as total_amount,
    AVG(o.total_amount) as avg_order_amount,
    MAX(o.created_at) as last_order_date
FROM users u
INNER JOIN orders o ON u.user_id = o.user_id
WHERE u.created_at >= '2023-01-01'
    AND u.status = 'active'
    AND EXISTS (
        SELECT 1 FROM order_items oi 
        INNER JOIN products p ON oi.product_id = p.product_id
        WHERE oi.order_id = o.order_id 
            AND p.category_id IN (1,2,3,4,5)
    )
GROUP BY u.user_id, u.username, u.email
ORDER BY total_amount DESC
LIMIT 100;
```

**ç¬¬å››æ­¥ï¼šPythonæ€§èƒ½æµ‹è¯•æ¡†æ¶**

```python
import time
import mysql.connector
from contextlib import contextmanager
import statistics
from typing import List, Dict

class SQLPerformanceAnalyzer:
    def __init__(self, db_config: Dict):
        self.db_config = db_config
        self.performance_log = []
    
    @contextmanager
    def get_db_connection(self):
        """æ•°æ®åº“è¿æ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        conn = mysql.connector.connect(**self.db_config)
        try:
            yield conn
        finally:
            conn.close()
    
    def measure_query_performance(self, query: str, params: tuple = None, 
                                iterations: int = 5) -> Dict:
        """æµ‹é‡SQLæŸ¥è¯¢æ€§èƒ½"""
        execution_times = []
        
        with self.get_db_connection() as conn:
            cursor = conn.cursor()
            
            # é¢„çƒ­æŸ¥è¯¢ï¼ˆé¿å…å†·å¯åŠ¨å½±å“ï¼‰
            cursor.execute(query, params or ())
            cursor.fetchall()
            
            # æ­£å¼æ€§èƒ½æµ‹è¯•
            for i in range(iterations):
                start_time = time.time()
                cursor.execute(query, params or ())
                results = cursor.fetchall()
                end_time = time.time()
                
                execution_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                execution_times.append(execution_time)
                
                print(f"æ‰§è¡Œ {i+1}/{iterations}: {execution_time:.2f}ms")
            
            cursor.close()
        
        # ç»Ÿè®¡åˆ†æ
        performance_stats = {
            'query': query[:100] + "..." if len(query) > 100 else query,
            'iterations': iterations,
            'execution_times': execution_times,
            'min_time': min(execution_times),
            'max_time': max(execution_times),
            'avg_time': statistics.mean(execution_times),
            'median_time': statistics.median(execution_times),
            'std_dev': statistics.stdev(execution_times) if len(execution_times) > 1 else 0,
            'result_count': len(results) if 'results' in locals() else 0
        }
        
        self.performance_log.append(performance_stats)
        return performance_stats
    
    def compare_query_versions(self, original_query: str, optimized_queries: List[str], 
                             params: tuple = None) -> Dict:
        """æ¯”è¾ƒä¸åŒç‰ˆæœ¬çš„SQLæŸ¥è¯¢æ€§èƒ½"""
        print("å¼€å§‹SQLæŸ¥è¯¢æ€§èƒ½å¯¹æ¯”æµ‹è¯•...")
        
        # æµ‹è¯•åŸå§‹æŸ¥è¯¢
        print(f"\næµ‹è¯•åŸå§‹æŸ¥è¯¢...")
        original_stats = self.measure_query_performance(original_query, params)
        
        # æµ‹è¯•ä¼˜åŒ–åçš„æŸ¥è¯¢
        optimized_stats = []
        for i, optimized_query in enumerate(optimized_queries, 1):
            print(f"\næµ‹è¯•ä¼˜åŒ–æ–¹æ¡ˆ {i}...")
            stats = self.measure_query_performance(optimized_query, params)
            optimized_stats.append(stats)
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        comparison_report = {
            'original': original_stats,
            'optimized_versions': optimized_stats,
            'improvements': []
        }
        
        # è®¡ç®—æ€§èƒ½æå‡
        original_avg = original_stats['avg_time']
        for i, opt_stats in enumerate(optimized_stats, 1):
            improvement_ratio = (original_avg - opt_stats['avg_time']) / original_avg * 100
            comparison_report['improvements'].append({
                'version': i,
                'avg_time': opt_stats['avg_time'],
                'improvement_percentage': improvement_ratio,
                'performance_gain': f"{improvement_ratio:.1f}%" if improvement_ratio > 0 else f"{-improvement_ratio:.1f}% slower"
            })
        
        return comparison_report
    
    def generate_optimization_report(self, comparison: Dict) -> str:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        report = []
        report.append("=== SQLæŸ¥è¯¢ä¼˜åŒ–æŠ¥å‘Š ===\n")
        
        # åŸå§‹æ€§èƒ½
        original = comparison['original']
        report.append(f"åŸå§‹æŸ¥è¯¢æ€§èƒ½:")
        report.append(f"  å¹³å‡æ‰§è¡Œæ—¶é—´: {original['avg_time']:.2f}ms")
        report.append(f"  æœ€å°æ‰§è¡Œæ—¶é—´: {original['min_time']:.2f}ms")
        report.append(f"  æœ€å¤§æ‰§è¡Œæ—¶é—´: {original['max_time']:.2f}ms")
        report.append(f"  æ ‡å‡†å·®: {original['std_dev']:.2f}ms")
        
        # ä¼˜åŒ–ç»“æœ
        report.append(f"\nä¼˜åŒ–ç»“æœ:")
        best_improvement = 0
        best_version = 0
        
        for improvement in comparison['improvements']:
            version = improvement['version']
            avg_time = improvement['avg_time']
            gain = improvement['performance_gain']
            
            report.append(f"  ä¼˜åŒ–æ–¹æ¡ˆ {version}:")
            report.append(f"    å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_time:.2f}ms")
            report.append(f"    æ€§èƒ½æå‡: {gain}")
            
            if improvement['improvement_percentage'] > best_improvement:
                best_improvement = improvement['improvement_percentage']
                best_version = version
        
        # æ¨èæ–¹æ¡ˆ
        if best_version > 0:
            report.append(f"\næ¨èæ–¹æ¡ˆ: ä¼˜åŒ–æ–¹æ¡ˆ {best_version}")
            report.append(f"æ€§èƒ½æå‡: {best_improvement:.1f}%")
        
        return "\n".join(report)

# ä½¿ç”¨ç¤ºä¾‹
def sql_optimization_test():
    """SQLä¼˜åŒ–æµ‹è¯•ç¤ºä¾‹"""
    
    # æ•°æ®åº“é…ç½®
    db_config = {
        'host': 'localhost',
        'database': 'test_db',
        'user': 'test_user', 
        'password': 'test_password'
    }
    
    # åˆ›å»ºæ€§èƒ½åˆ†æå™¨
    analyzer = SQLPerformanceAnalyzer(db_config)
    
    # åŸå§‹æ…¢SQL
    original_query = """
    SELECT 
        u.username,
        u.email,
        COUNT(o.order_id) as total_orders,
        SUM(o.total_amount) as total_amount,
        AVG(o.total_amount) as avg_order_amount,
        MAX(o.created_at) as last_order_date
    FROM users u
    LEFT JOIN orders o ON u.user_id = o.user_id
    LEFT JOIN order_items oi ON o.order_id = oi.order_id
    LEFT JOIN products p ON oi.product_id = p.product_id
    WHERE u.created_at >= '2023-01-01'
        AND u.status = 'active'
        AND p.category_id IN (1,2,3,4,5)
    GROUP BY u.user_id, u.username, u.email
    HAVING COUNT(o.order_id) > 0
    ORDER BY total_amount DESC
    LIMIT 100
    """
    
    # ä¼˜åŒ–åçš„SQLç‰ˆæœ¬
    optimized_queries = [
        # ä¼˜åŒ–æ–¹æ¡ˆ1ï¼šä½¿ç”¨CTEå’ŒINNER JOIN
        """
        WITH active_users AS (
            SELECT user_id, username, email
            FROM users 
            WHERE created_at >= '2023-01-01' AND status = 'active'
        ),
        valid_orders AS (
            SELECT DISTINCT o.user_id, o.order_id, o.total_amount, o.created_at
            FROM orders o
            INNER JOIN order_items oi ON o.order_id = oi.order_id
            INNER JOIN products p ON oi.product_id = p.product_id
            WHERE p.category_id IN (1,2,3,4,5)
        )
        SELECT 
            au.username, au.email,
            COUNT(vo.order_id) as total_orders,
            SUM(vo.total_amount) as total_amount,
            AVG(vo.total_amount) as avg_order_amount,
            MAX(vo.created_at) as last_order_date
        FROM active_users au
        INNER JOIN valid_orders vo ON au.user_id = vo.user_id
        GROUP BY au.user_id, au.username, au.email
        ORDER BY total_amount DESC
        LIMIT 100
        """,
        
        # ä¼˜åŒ–æ–¹æ¡ˆ2ï¼šä½¿ç”¨EXISTS
        """
        SELECT 
            u.username, u.email,
            COUNT(o.order_id) as total_orders,
            SUM(o.total_amount) as total_amount,
            AVG(o.total_amount) as avg_order_amount,
            MAX(o.created_at) as last_order_date
        FROM users u
        INNER JOIN orders o ON u.user_id = o.user_id
        WHERE u.created_at >= '2023-01-01'
            AND u.status = 'active'
            AND EXISTS (
                SELECT 1 FROM order_items oi 
                INNER JOIN products p ON oi.product_id = p.product_id
                WHERE oi.order_id = o.order_id 
                    AND p.category_id IN (1,2,3,4,5)
            )
        GROUP BY u.user_id, u.username, u.email
        ORDER BY total_amount DESC
        LIMIT 100
        """
    ]
    
    # æ‰§è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•
    comparison = analyzer.compare_query_versions(original_query, optimized_queries)
    
    # ç”Ÿæˆå¹¶æ‰“å°æŠ¥å‘Š
    report = analyzer.generate_optimization_report(comparison)
    print(report)
    
    return comparison

if __name__ == "__main__":
    sql_optimization_test()
```

**Result (ç»“æœ)**:
é€šè¿‡ç³»ç»ŸåŒ–çš„SQLä¼˜åŒ–ï¼Œæˆ‘å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼š

1. **æ‰§è¡Œæ—¶é—´ä¼˜åŒ–**: æŸ¥è¯¢æ—¶é—´ä»5.2ç§’ä¼˜åŒ–åˆ°380msï¼Œæ€§èƒ½æå‡äº†93%
2. **ç´¢å¼•ç­–ç•¥æˆåŠŸ**: åˆ›å»ºçš„å¤åˆç´¢å¼•ä½¿æŸ¥è¯¢é¿å…äº†å…¨è¡¨æ‰«æ
3. **æŸ¥è¯¢é‡æ„æ•ˆæœ**: ä½¿ç”¨CTEå’ŒINNER JOINæ›¿ä»£å¤æ‚çš„LEFT JOINï¼Œå‡å°‘äº†æ•°æ®å¤„ç†é‡
4. **æµ‹è¯•æ¡†æ¶å»ºç«‹**: å»ºç«‹äº†SQLæ€§èƒ½æµ‹è¯•æ¡†æ¶ï¼Œå¯ä»¥è‡ªåŠ¨åŒ–å¯¹æ¯”ä¸åŒä¼˜åŒ–æ–¹æ¡ˆ

**ä¼˜åŒ–æ€»ç»“**:
- **ç´¢å¼•ä¼˜åŒ–**: è§£å†³äº†80%çš„æ€§èƒ½é—®é¢˜
- **æŸ¥è¯¢é‡æ„**: è¿›ä¸€æ­¥æå‡äº†15%çš„æ€§èƒ½  
- **æ‰§è¡Œè®¡åˆ’åˆ†æ**: ç¡®ä¿äº†ä¼˜åŒ–æ–¹å‘çš„æ­£ç¡®æ€§
- **æ€§èƒ½ç›‘æ§**: å»ºç«‹äº†æŒç»­çš„æ€§èƒ½ç›‘æ§æœºåˆ¶

è¿™å¥—SQLä¼˜åŒ–æ–¹æ³•è®ºåæ¥è¢«æ¨å¹¿åˆ°äº†æ•´ä¸ªå›¢é˜Ÿï¼Œå¸®åŠ©è§£å†³äº†å¤šä¸ªé¡¹ç›®ä¸­çš„æ•°æ®åº“æ€§èƒ½é—®é¢˜ã€‚

---

## ğŸ“¡ è®¡ç®—æœºç½‘ç»œä¸“é¢˜ STARç­”æ¡ˆ

### â­â­â­ å¦‚ä½•è¿›è¡Œç½‘ç»œåè®®çš„æµ‹è¯•å’Œè°ƒè¯•ï¼Ÿ

**é—®é¢˜**: åœ¨æ¥å£æµ‹è¯•ä¸­é‡åˆ°ç½‘ç»œåè®®å±‚é¢çš„é—®é¢˜ï¼Œä½ ä¼šå¦‚ä½•è¿›è¡Œåˆ†æå’Œè°ƒè¯•ï¼Ÿ

**STARæ¡†æ¶å›ç­”**:

**Situation (æƒ…æ™¯)**: 
åœ¨ä¸€æ¬¡å¾®æœåŠ¡æ¥å£æµ‹è¯•ä¸­ï¼Œæˆ‘å‘ç°æŸä¸ªAPIçš„å“åº”æ—¶é—´ä¸ç¨³å®šï¼Œæœ‰æ—¶200msï¼Œæœ‰æ—¶è¶…è¿‡3ç§’ï¼Œè€Œä¸”å¶å°”ä¼šå‡ºç°è¿æ¥è¶…æ—¶çš„æƒ…å†µï¼Œéœ€è¦ä»ç½‘ç»œåè®®å±‚é¢åˆ†æé—®é¢˜ã€‚

**Task (ä»»åŠ¡)**: 
éœ€è¦åˆ†æç½‘ç»œé€šä¿¡è¿‡ç¨‹ï¼Œè¯†åˆ«ç½‘ç»œåè®®å±‚é¢çš„é—®é¢˜ï¼Œå¹¶åˆ¶å®šç›¸åº”çš„æµ‹è¯•ç­–ç•¥å’Œè§£å†³æ–¹æ¡ˆã€‚

**Action (è¡ŒåŠ¨)**:
æˆ‘ä½¿ç”¨äº†å¤šå±‚æ¬¡çš„ç½‘ç»œåè®®åˆ†ææ–¹æ³•ï¼š

```python
import socket
import time
import requests
import subprocess
import json
from typing import Dict, List, Tuple
import scapy.all as scapy
from scapy.layers.inet import IP, TCP, ICMP
from scapy.layers.http import HTTPRequest, HTTPResponse

class NetworkProtocolAnalyzer:
    def __init__(self):
        self.capture_results = []
        self.timing_results = []
    
    def tcp_connection_test(self, host: str, port: int, timeout: int = 10) -> Dict:
        """TCPè¿æ¥æµ‹è¯•"""
        print(f"æµ‹è¯•TCPè¿æ¥åˆ° {host}:{port}")
        
        result = {
            'host': host,
            'port': port,
            'tcp_connect_time': None,
            'connection_successful': False,
            'error': None
        }
        
        try:
            start_time = time.time()
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(timeout)
            
            connection_result = sock.connect_ex((host, port))
            connect_time = time.time() - start_time
            
            if connection_result == 0:
                result['connection_successful'] = True
                result['tcp_connect_time'] = connect_time * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                print(f"TCPè¿æ¥æˆåŠŸï¼Œè€—æ—¶: {result['tcp_connect_time']:.2f}ms")
            else:
                result['error'] = f"TCPè¿æ¥å¤±è´¥ï¼Œé”™è¯¯ä»£ç : {connection_result}"
                print(result['error'])
            
            sock.close()
            
        except Exception as e:
            result['error'] = str(e)
            print(f"TCPè¿æ¥å¼‚å¸¸: {e}")
        
        return result
    
    def http_timing_analysis(self, url: str, iterations: int = 5) -> Dict:
        """HTTPè¯·æ±‚æ—¶åºåˆ†æ"""
        print(f"HTTPæ—¶åºåˆ†æ: {url}")
        
        timings = []
        
        for i in range(iterations):
            try:
                start_time = time.time()
                response = requests.get(url, timeout=30)
                total_time = time.time() - start_time
                
                # è·å–è¯¦ç»†çš„æ—¶åºä¿¡æ¯
                timing_info = {
                    'iteration': i + 1,
                    'total_time': total_time * 1000,
                    'status_code': response.status_code,
                    'response_size': len(response.content),
                    'headers': dict(response.headers)
                }
                
                # å¦‚æœæœ‰requests-toolbeltï¼Œå¯ä»¥è·å–æ›´è¯¦ç»†çš„æ—¶åº
                if hasattr(response, 'elapsed'):
                    timing_info['requests_elapsed'] = response.elapsed.total_seconds() * 1000
                
                timings.append(timing_info)
                print(f"è¯·æ±‚ {i+1}: {timing_info['total_time']:.2f}ms, "
                     f"çŠ¶æ€ç : {timing_info['status_code']}")
                
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                
            except Exception as e:
                timings.append({
                    'iteration': i + 1,
                    'error': str(e),
                    'total_time': None
                })
                print(f"è¯·æ±‚ {i+1} å¤±è´¥: {e}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        successful_timings = [t['total_time'] for t in timings if t.get('total_time')]
        
        analysis_result = {
            'url': url,
            'total_requests': iterations,
            'successful_requests': len(successful_timings),
            'failure_rate': (iterations - len(successful_timings)) / iterations * 100,
            'timings': timings
        }
        
        if successful_timings:
            analysis_result.update({
                'min_time': min(successful_timings),
                'max_time': max(successful_timings),
                'avg_time': sum(successful_timings) / len(successful_timings),
                'time_variance': max(successful_timings) - min(successful_timings)
            })
        
        return analysis_result
    
    def packet_capture_analysis(self, interface: str, filter_expr: str, 
                               capture_duration: int = 30) -> List[Dict]:
        """ç½‘ç»œåŒ…æ•è·åˆ†æ"""
        print(f"å¼€å§‹ç½‘ç»œåŒ…æ•è·ï¼Œæ¥å£: {interface}, è¿‡æ»¤: {filter_expr}, æ—¶é•¿: {capture_duration}s")
        
        captured_packets = []
        
        def packet_handler(packet):
            """åŒ…å¤„ç†å›è°ƒå‡½æ•°"""
            packet_info = {
                'timestamp': float(packet.time),
                'protocol': packet.name if hasattr(packet, 'name') else 'Unknown',
                'size': len(packet)
            }
            
            # IPå±‚ä¿¡æ¯
            if packet.haslayer(IP):
                ip_layer = packet[IP]
                packet_info.update({
                    'src_ip': ip_layer.src,
                    'dst_ip': ip_layer.dst,
                    'ttl': ip_layer.ttl,
                    'protocol_type': ip_layer.proto
                })
            
            # TCPå±‚ä¿¡æ¯
            if packet.haslayer(TCP):
                tcp_layer = packet[TCP]
                packet_info.update({
                    'src_port': tcp_layer.sport,
                    'dst_port': tcp_layer.dport,
                    'flags': tcp_layer.flags,
                    'seq': tcp_layer.seq,
                    'ack': tcp_layer.ack
                })
            
            # HTTPä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if packet.haslayer(HTTPRequest):
                http_req = packet[HTTPRequest]
                packet_info.update({
                    'http_method': http_req.Method.decode() if http_req.Method else None,
                    'http_path': http_req.Path.decode() if http_req.Path else None,
                    'http_host': http_req.Host.decode() if http_req.Host else None
                })
            
            if packet.haslayer(HTTPResponse):
                http_resp = packet[HTTPResponse]
                packet_info.update({
                    'http_status': http_resp.Status_Code.decode() if http_resp.Status_Code else None,
                    'http_reason': http_resp.Reason_Phrase.decode() if http_resp.Reason_Phrase else None
                })
            
            captured_packets.append(packet_info)
        
        try:
            # å¼€å§‹æ•è·
            scapy.sniff(
                iface=interface,
                filter=filter_expr,
                prn=packet_handler,
                timeout=capture_duration,
                store=0  # ä¸å­˜å‚¨åŒ…åœ¨å†…å­˜ä¸­ï¼Œç›´æ¥å¤„ç†
            )
            
            print(f"æ•è·å®Œæˆï¼Œå…±æ•è· {len(captured_packets)} ä¸ªæ•°æ®åŒ…")
            
        except Exception as e:
            print(f"åŒ…æ•è·å¼‚å¸¸: {e}")
        
        return captured_packets
    
    def analyze_connection_issues(self, host: str, port: int, url: str) -> Dict:
        """ç»¼åˆè¿æ¥é—®é¢˜åˆ†æ"""
        print(f"å¼€å§‹ç»¼åˆè¿æ¥åˆ†æ: {host}:{port}")
        
        analysis_result = {
            'target': f"{host}:{port}",
            'url': url,
            'timestamp': time.time(),
            'tests_performed': []
        }
        
        # 1. TCPè¿æ¥æµ‹è¯•
        print("\n1. TCPè¿æ¥æµ‹è¯•")
        tcp_result = self.tcp_connection_test(host, port)
        analysis_result['tcp_test'] = tcp_result
        analysis_result['tests_performed'].append('TCPè¿æ¥æµ‹è¯•')
        
        # 2. HTTPæ—¶åºåˆ†æ
        print("\n2. HTTPæ—¶åºåˆ†æ")
        http_result = self.http_timing_analysis(url)
        analysis_result['http_timing'] = http_result
        analysis_result['tests_performed'].append('HTTPæ—¶åºåˆ†æ')
        
        # 3. ç½‘ç»œè·¯å¾„æµ‹è¯•ï¼ˆtracerouteï¼‰
        print("\n3. ç½‘ç»œè·¯å¾„æµ‹è¯•")
        traceroute_result = self.traceroute_analysis(host)
        analysis_result['traceroute'] = traceroute_result
        analysis_result['tests_performed'].append('ç½‘ç»œè·¯å¾„æµ‹è¯•')
        
        # 4. DNSè§£ææµ‹è¯•
        print("\n4. DNSè§£ææµ‹è¯•")
        dns_result = self.dns_resolution_test(host)
        analysis_result['dns_test'] = dns_result
        analysis_result['tests_performed'].append('DNSè§£ææµ‹è¯•')
        
        # 5. ç”Ÿæˆé—®é¢˜è¯Šæ–­
        analysis_result['diagnosis'] = self.generate_diagnosis(analysis_result)
        
        return analysis_result
    
    def traceroute_analysis(self, host: str) -> Dict:
        """ç½‘ç»œè·¯å¾„è·Ÿè¸ªåˆ†æ"""
        try:
            if subprocess.run(['which', 'traceroute'], 
                            capture_output=True).returncode == 0:
                # Linux/Macä½¿ç”¨traceroute
                result = subprocess.run(['traceroute', '-n', host], 
                                      capture_output=True, text=True, timeout=30)
                output = result.stdout
            else:
                # Windowsä½¿ç”¨tracert
                result = subprocess.run(['tracert', '-d', host], 
                                      capture_output=True, text=True, timeout=30)
                output = result.stdout
            
            return {
                'command_successful': result.returncode == 0,
                'output': output,
                'hop_count': len([line for line in output.split('\n') 
                                if line.strip() and not line.startswith('traceroute')])
            }
        except Exception as e:
            return {
                'command_successful': False,
                'error': str(e)
            }
    
    def dns_resolution_test(self, host: str) -> Dict:
        """DNSè§£ææµ‹è¯•"""
        try:
            start_time = time.time()
            ip_addresses = socket.gethostbyname_ex(host)
            resolution_time = (time.time() - start_time) * 1000
            
            return {
                'resolution_successful': True,
                'resolution_time': resolution_time,
                'hostname': ip_addresses[0],
                'aliases': ip_addresses[1],
                'ip_addresses': ip_addresses[2]
            }
        except Exception as e:
            return {
                'resolution_successful': False,
                'error': str(e)
            }
    
    def generate_diagnosis(self, analysis_result: Dict) -> Dict:
        """ç”Ÿæˆé—®é¢˜è¯Šæ–­"""
        issues = []
        recommendations = []
        
        # TCPè¿æ¥åˆ†æ
        tcp_test = analysis_result.get('tcp_test', {})
        if not tcp_test.get('connection_successful'):
            issues.append("TCPè¿æ¥å¤±è´¥")
            recommendations.append("æ£€æŸ¥ç½‘ç»œè¿é€šæ€§å’Œé˜²ç«å¢™è®¾ç½®")
        elif tcp_test.get('tcp_connect_time', 0) > 1000:  # è¶…è¿‡1ç§’
            issues.append(f"TCPè¿æ¥ç¼“æ…¢: {tcp_test['tcp_connect_time']:.2f}ms")
            recommendations.append("æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿå’ŒæœåŠ¡å™¨è´Ÿè½½")
        
        # HTTPæ—¶åºåˆ†æ
        http_timing = analysis_result.get('http_timing', {})
        if http_timing.get('failure_rate', 0) > 10:  # å¤±è´¥ç‡è¶…è¿‡10%
            issues.append(f"HTTPè¯·æ±‚å¤±è´¥ç‡è¿‡é«˜: {http_timing['failure_rate']:.1f}%")
            recommendations.append("æ£€æŸ¥æœåŠ¡ç«¯ç¨³å®šæ€§å’Œç½‘ç»œè´¨é‡")
        
        if http_timing.get('time_variance', 0) > 2000:  # æ—¶é—´å·®å¼‚è¶…è¿‡2ç§’
            issues.append(f"å“åº”æ—¶é—´ä¸ç¨³å®šï¼Œæœ€å¤§å·®å¼‚: {http_timing['time_variance']:.2f}ms")
            recommendations.append("åˆ†ææœåŠ¡ç«¯å¤„ç†é€»è¾‘å’Œè´Ÿè½½åˆ†å¸ƒ")
        
        # DNSè§£æåˆ†æ
        dns_test = analysis_result.get('dns_test', {})
        if not dns_test.get('resolution_successful'):
            issues.append("DNSè§£æå¤±è´¥")
            recommendations.append("æ£€æŸ¥DNSé…ç½®å’Œç½‘ç»œè¿æ¥")
        elif dns_test.get('resolution_time', 0) > 100:  # DNSè§£æè¶…è¿‡100ms
            issues.append(f"DNSè§£æç¼“æ…¢: {dns_test['resolution_time']:.2f}ms")
            recommendations.append("è€ƒè™‘ä½¿ç”¨æ›´å¿«çš„DNSæœåŠ¡å™¨")
        
        return {
            'issues_found': len(issues),
            'identified_issues': issues,
            'recommendations': recommendations,
            'overall_health': 'Good' if len(issues) == 0 else 'Poor' if len(issues) > 2 else 'Fair'
        }

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•è„šæœ¬
def network_protocol_debugging():
    """ç½‘ç»œåè®®è°ƒè¯•ç¤ºä¾‹"""
    
    analyzer = NetworkProtocolAnalyzer()
    
    # æµ‹è¯•ç›®æ ‡
    test_cases = [
        {
            'name': 'ç”Ÿäº§APIæœåŠ¡å™¨',
            'host': 'api.example.com',
            'port': 443,
            'url': 'https://api.example.com/health'
        },
        {
            'name': 'æµ‹è¯•ç¯å¢ƒ',
            'host': 'test-api.example.com', 
            'port': 80,
            'url': 'http://test-api.example.com/status'
        }
    ]
    
    print("å¼€å§‹ç½‘ç»œåè®®åˆ†æ...")
    
    for test_case in test_cases:
        print(f"\n{'='*50}")
        print(f"åˆ†æç›®æ ‡: {test_case['name']}")
        print(f"{'='*50}")
        
        # æ‰§è¡Œç»¼åˆåˆ†æ
        result = analyzer.analyze_connection_issues(
            test_case['host'],
            test_case['port'], 
            test_case['url']
        )
        
        # è¾“å‡ºè¯Šæ–­ç»“æœ
        diagnosis = result['diagnosis']
        print(f"\nè¯Šæ–­ç»“æœ:")
        print(f"  æ•´ä½“å¥åº·çŠ¶å†µ: {diagnosis['overall_health']}")
        print(f"  å‘ç°é—®é¢˜æ•°é‡: {diagnosis['issues_found']}")
        
        if diagnosis['identified_issues']:
            print(f"  è¯†åˆ«çš„é—®é¢˜:")
            for issue in diagnosis['identified_issues']:
                print(f"    - {issue}")
        
        if diagnosis['recommendations']:
            print(f"  å»ºè®®çš„è§£å†³æ–¹æ¡ˆ:")
            for rec in diagnosis['recommendations']:
                print(f"    - {rec}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
        with open(f"network_analysis_{test_case['name'].replace(' ', '_')}.json", 'w') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

if __name__ == "__main__":
    network_protocol_debugging()
```

**é«˜çº§ç½‘ç»œè°ƒè¯•å·¥å…·é›†æˆ**:

```bash
#!/bin/bash
# ç½‘ç»œåè®®è°ƒè¯•è„šæœ¬

TARGET_HOST="$1"
TARGET_PORT="$2"

if [ -z "$TARGET_HOST" ] || [ -z "$TARGET_PORT" ]; then
    echo "ç”¨æ³•: $0 <ç›®æ ‡ä¸»æœº> <ç›®æ ‡ç«¯å£>"
    exit 1
fi

echo "å¼€å§‹ç½‘ç»œåè®®è°ƒè¯•: $TARGET_HOST:$TARGET_PORT"

# 1. åŸºç¡€è¿é€šæ€§æµ‹è¯•
echo "1. Pingæµ‹è¯•"
ping -c 4 "$TARGET_HOST"

# 2. ç«¯å£è¿é€šæ€§æµ‹è¯•
echo -e "\n2. ç«¯å£è¿é€šæ€§æµ‹è¯•"
nc -zv "$TARGET_HOST" "$TARGET_PORT"

# 3. TCPè¿æ¥æ—¶åºæµ‹è¯•
echo -e "\n3. TCPè¿æ¥æ—¶åº"
time telnet "$TARGET_HOST" "$TARGET_PORT" </dev/null 2>/dev/null

# 4. ç½‘ç»œè·¯å¾„è·Ÿè¸ª
echo -e "\n4. è·¯å¾„è·Ÿè¸ª"
traceroute "$TARGET_HOST"

# 5. DNSè§£ææ—¶é—´
echo -e "\n5. DNSè§£ææµ‹è¯•"
time nslookup "$TARGET_HOST"

# 6. TCPçŠ¶æ€ç›‘æ§
echo -e "\n6. TCPè¿æ¥çŠ¶æ€"
netstat -an | grep ":$TARGET_PORT"

# 7. ç½‘ç»œåŒ…æ•è·ï¼ˆéœ€è¦rootæƒé™ï¼‰
if [ "$EUID" -eq 0 ]; then
    echo -e "\n7. ç½‘ç»œåŒ…æ•è·ï¼ˆ10ç§’ï¼‰"
    timeout 10s tcpdump -i any host "$TARGET_HOST" and port "$TARGET_PORT" -c 20
else
    echo -e "\n7. è·³è¿‡ç½‘ç»œåŒ…æ•è·ï¼ˆéœ€è¦rootæƒé™ï¼‰"
fi

# 8. SSL/TLSæµ‹è¯•ï¼ˆå¦‚æœæ˜¯HTTPSï¼‰
if [ "$TARGET_PORT" -eq 443 ]; then
    echo -e "\n8. SSL/TLSæµ‹è¯•"
    openssl s_client -connect "$TARGET_HOST:$TARGET_PORT" -servername "$TARGET_HOST" </dev/null 2>/dev/null | grep -E 'Protocol|Cipher'
fi

echo -e "\nç½‘ç»œåè®®è°ƒè¯•å®Œæˆ"
```

**Result (ç»“æœ)**:
é€šè¿‡ç³»ç»ŸåŒ–çš„ç½‘ç»œåè®®åˆ†æï¼Œæˆ‘æˆåŠŸå®šä½äº†é—®é¢˜ï¼š

1. **é—®é¢˜å®šä½**: å‘ç°æ˜¯è´Ÿè½½å‡è¡¡å™¨çš„è¿æ¥æ± è®¾ç½®ä¸å½“ï¼Œå¯¼è‡´è¿æ¥å¤ç”¨ç‡ä½
2. **æ ¹å› åˆ†æ**: TCPè¿æ¥æ—¶é—´æ­£å¸¸ï¼Œä½†HTTP Keep-Aliveæ²¡æœ‰æ­£ç¡®é…ç½®
3. **è§£å†³æ–¹æ¡ˆ**: 
   - è°ƒæ•´è´Ÿè½½å‡è¡¡å™¨çš„è¿æ¥æ± å¤§å°
   - é…ç½®HTTP Keep-Aliveå‚æ•°
   - ä¼˜åŒ–æœåŠ¡ç«¯è¿æ¥å¤„ç†é€»è¾‘
4. **æ€§èƒ½æ”¹å–„**: APIå“åº”æ—¶é—´ç¨³å®šåœ¨200mså·¦å³ï¼Œè¿æ¥è¶…æ—¶é—®é¢˜å®Œå…¨è§£å†³

**å»ºç«‹çš„ç½‘ç»œè°ƒè¯•å·¥å…·é“¾**:
- Pythonè‡ªåŠ¨åŒ–åˆ†æå·¥å…·ï¼šæä¾›è¯¦ç»†çš„æ—¶åºå’Œç»Ÿè®¡åˆ†æ
- Shellè„šæœ¬å¿«é€Ÿè¯Šæ–­ï¼šç”¨äºå¿«é€Ÿé—®é¢˜å®šä½
- ç½‘ç»œåŒ…æ•è·åˆ†æï¼šæ·±å…¥åè®®å±‚é¢çš„é—®é¢˜è¯Šæ–­
- æŒç»­ç›‘æ§æœºåˆ¶ï¼šåŠæ—¶å‘ç°å’Œé¢„è­¦ç½‘ç»œé—®é¢˜

è¿™å¥—ç½‘ç»œåè®®è°ƒè¯•æ–¹æ³•åæ¥æˆä¸ºäº†å›¢é˜Ÿçš„æ ‡å‡†æµç¨‹ï¼Œå¤§å¤§æå‡äº†ç½‘ç»œç›¸å…³é—®é¢˜çš„æ’æŸ¥æ•ˆç‡ã€‚